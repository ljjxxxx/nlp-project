{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RNN (Recurrent Neural Network)\n",
    "**RNN (Recurrent Neural Network)** is a type of neural network designed for processing **sequential data**, such as time series, text, or speech. Unlike traditional feedforward networks (e.g., CNN), RNNs have memory through **hidden states**, allowing to retain information from previous times steps and influence future outputs.\n",
    "\n",
    "## Core Idea of RNN\n",
    "RNNs use **recurrent connections**, meaning the output at each time step depends on:\n",
    "* The **current input** (e.g., a word in a sentence).\n",
    "* The **previous hidden state** (memory of past data). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ece1657887f7c92a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<UNK>': 1, 'i': 2, 'love': 3, 'this': 4, 'movie!': 5, 'film': 6, 'is': 7, 'terrible.': 8, 'great': 9, 'story!': 10, 'worst': 11, 'acting': 12, 'ever.': 13}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Example\n",
    "texts = [\"I love this movie!\", \n",
    "         \"This film is terrible.\", \n",
    "         \"Great story!\", \n",
    "         \"Worst acting ever.\"]\n",
    "labels = [1, 0, 1, 0]  # 1=positive, 0=negative\n",
    "\n",
    "# vocab\n",
    "word_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1}  # padding and unknown word\n",
    "for text in texts:\n",
    "    for word in text.lower().split():\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "print(word_to_idx)\n",
    "\n",
    "# Converts text into an index sequence\n",
    "def text_to_indices(text, word_to_idx):\n",
    "    return [word_to_idx.get(word.lower(), word_to_idx[\"<UNK>\"]) \n",
    "            for word in text.split()]\n",
    "\n",
    "# Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word_to_idx):\n",
    "        self.texts = [text_to_indices(text, word_to_idx) for text in texts]\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "# pad\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts_padded = torch.nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    return texts_padded, torch.stack(labels)\n",
    "\n",
    "dataset = TextDataset(texts, labels, word_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-06T07:11:19.427725Z",
     "start_time": "2025-04-06T07:11:19.378914Z"
    }
   },
   "id": "508428b464ff8a3e",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # [batch, seq_len, embed_dim]\n",
    "        _, hidden = self.rnn(embedded)  # hidden: [1, batch, hidden_dim]\n",
    "        output = self.fc(hidden.squeeze(0))  # [batch, output_dim]\n",
    "        return output\n",
    "\n",
    "# hyper-parameter\n",
    "VOCAB_SIZE = len(word_to_idx)\n",
    "EMBED_DIM = 16\n",
    "HIDDEN_DIM = 32\n",
    "OUTPUT_DIM = 1  \n",
    "\n",
    "model = SimpleRNN(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-06T07:11:23.827195Z",
     "start_time": "2025-04-06T07:11:23.804478Z"
    }
   },
   "id": "cd33dd57f13ecb91",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7415\n",
      "Epoch 2, Loss: 0.4603\n",
      "Epoch 3, Loss: 0.2376\n",
      "Epoch 4, Loss: 0.1139\n",
      "Epoch 5, Loss: 0.0552\n",
      "Epoch 6, Loss: 0.0289\n",
      "Epoch 7, Loss: 0.0168\n",
      "Epoch 8, Loss: 0.0107\n",
      "Epoch 9, Loss: 0.0074\n",
      "Epoch 10, Loss: 0.0054\n"
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(10):\n",
    "    for batch_texts, batch_labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_texts).squeeze(1)\n",
    "        loss = criterion(predictions, batch_labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-06T07:11:26.119737Z",
     "start_time": "2025-04-06T07:11:26.015770Z"
    }
   },
   "id": "73920f66289e3f2",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(model, word_to_idx, sentence):\n",
    "    model.eval()\n",
    "    indices = text_to_indices(sentence, word_to_idx)\n",
    "    tensor = torch.LongTensor(indices).unsqueeze(0)  # [1, seq_len]\n",
    "    with torch.no_grad():\n",
    "        output = torch.sigmoid(model(tensor))\n",
    "    return \"Positive\" if output.item() > 0.5 else \"Negative\"\n",
    "\n",
    "# Tests\n",
    "print(predict_sentiment(model, word_to_idx, \"This is great!\")) \n",
    "print(predict_sentiment(model, word_to_idx, \"I hate it\"))      "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-06T07:17:01.127291Z",
     "start_time": "2025-04-06T07:17:01.101167Z"
    }
   },
   "id": "ca5103b3f52a9e",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pros & Cons\n",
    "### Pros\n",
    "1. **Sequence Modeling**\n",
    "* RNNs are **natively designed** to process sequential data (e.g., time series, text, speech) by maintaining a **hidden state* that captures temporal dependencies.\n",
    "2. **Variable-Length Inputs**\n",
    "* Unlike CNNs or feedforward networks, RNNs can handle **inputs of varying lengths** (e.g., sentences of different lengths).\n",
    "3. **Parameter Sharing**\n",
    "* The same weights are reused across time steps, reducing the number of parameters compared to fully connected networks.\n",
    "4. **Memory of Past Information**\n",
    "* Hidden states act as a \"memory\" of previous inputs.\n",
    "\n",
    "### Cons\n",
    "1. **Vanishing/Exploding Gradients**\n",
    "* In long sequences, gradients can **vanish** (become too small) or **explode** (become too large), making training difficult.\n",
    "2. **Short-Term Memory**\n",
    "* Standard RNNs struggle to retain **long-range dependencies** (e.g., a word at the start of a paragraph influencing the end).\n",
    "3. **Computationally Slow**\n",
    "* RNNs process data **sequentially** (one time step at a time), preventing parallelization.\n",
    "4. **Difficulty with Very Long Sequences**\n",
    "* Even with LSTM/GRU, extremely long sequences (e.g., books, hour-long audio) remain challenging."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d98b6bc1f4a6123"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
