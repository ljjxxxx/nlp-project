{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparison of jieba, spaCy, and NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Core Features & Focus\n",
    "* jieba: Chinese text segmentation (exact/full/search modes), POS tagging, keyword extraction. Optimized for **Chinese text processing**.\n",
    "* spaCy: Tokenization, POS tagging, dependency parsing, NER, text classification, lemmatization. Industrial-strength **multilingual NLP**.\n",
    "* NLTK: Tokenization, POS tagging, parsing, stemming, corpus tools, classic NLP algorithms. **Education and research**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Language Support\n",
    "* jieba: **Chinese only**.\n",
    "* spaCy: Requires Chinese model, 20+ languages (English, German, French, etc.).\n",
    "* NLTK: Limited (needs external tools like Stanford Segmenter), Supports many languages via third-party resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* jieba: Best for Chinese text segmentation with high speed and simplicity.\n",
    "* spaCy: Ideal for production-ready multilingual NLP with deep learning integration.\n",
    "* NLTK: Perfect for education and algorithm experimentation.\n",
    "Choose based on your project's language, scale, and complexity! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T07:44:11.040656Z",
     "start_time": "2025-04-04T07:44:03.165159Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # 1. å»é™¤HTMLæ ‡ç­¾\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "\n",
    "    # 2. å¤„ç†ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # ä¿ç•™å­—æ¯å’Œç©ºæ ¼\n",
    "    text = re.sub(r'\\d+', '<NUM>', text)  # æ›¿æ¢æ•°å­—ä¸ºç‰¹æ®Šæ ‡è®°\n",
    "\n",
    "    # 3. è½¬æ¢ä¸ºå°å†™\n",
    "    text = text.lower()\n",
    "\n",
    "    # 4. åˆ†è¯\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 5. å»é™¤åœç”¨è¯\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # 6. è¯å¹²æå–\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    # 7. è¿‡æ»¤å•å­—ç¬¦\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def build_vocab(dataset_path, min_freq=5):\n",
    "    # ç»Ÿè®¡è¯é¢‘\n",
    "    word_freq = defaultdict(int)\n",
    "\n",
    "    # å‡è®¾æ•°æ®é›†æ˜¯æ¯è¡Œä¸€ä¸ªè¯„è®ºçš„æ–‡æœ¬æ–‡ä»¶\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            cleaned_tokens = clean_text(line)\n",
    "            for token in cleaned_tokens:\n",
    "                word_freq[token] += 1\n",
    "\n",
    "    # è¿‡æ»¤ä½é¢‘è¯\n",
    "    filtered_words = [word for word, freq in word_freq.items() if freq >= min_freq]\n",
    "\n",
    "    # æ·»åŠ ç‰¹æ®Šç¬¦å·\n",
    "    special_tokens = ['<PAD>', '<UNK>']\n",
    "    vocab = special_tokens + sorted(filtered_words)\n",
    "\n",
    "    # åˆ›å»ºæ˜ å°„å­—å…¸\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "    # # ä¿å­˜è¯è¡¨\n",
    "    # with open('vocab.pkl', 'wb') as f:\n",
    "    #     pickle.dump((word2idx, idx2word), f)\n",
    "\n",
    "    # åŒæ—¶ä¿å­˜å¯è¯»çš„TXTç‰ˆæœ¬\n",
    "    with open('vocab.txt', 'w', encoding='utf-8') as f:\n",
    "        for word in vocab:\n",
    "            f.write(f\"{word}\\n\")\n",
    "\n",
    "    return word2idx, idx2word\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "word2idx, idx2word = build_vocab('sample.positive.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
