{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3923, 0.6077, 0.0000, 0.0000],\n",
      "         [0.0638, 0.9362, 0.0000, 0.0000],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500]],\n",
      "\n",
      "        [[0.3104, 0.1882, 0.0395, 0.4620],\n",
      "         [0.1044, 0.4549, 0.0348, 0.4058],\n",
      "         [0.3452, 0.2657, 0.1628, 0.2263],\n",
      "         [0.0773, 0.1452, 0.1420, 0.6355]]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# word embedding\n",
    "batch_size = 2\n",
    "\n",
    "# word vocab size\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "model_dim = 8\n",
    "\n",
    "# max sequence length\n",
    "max_src_seq_len = 5\n",
    "max_tgt_seq_len = 5\n",
    "max_position_len = 5\n",
    "\n",
    "# src_len = torch.randint(2, 5, (batch_size,))\n",
    "# tgt_len = torch.randint(2, 5, (batch_size,))\n",
    "src_len = torch.Tensor([2, 4]).to(torch.int32)\n",
    "tgt_len = torch.Tensor([4, 3]).to(torch.int32)\n",
    "\n",
    "# word index sequence\n",
    "src_seq = torch.cat(\n",
    "    [torch.unsqueeze(F.pad(torch.randint(1, max_num_src_words, (L,)), (0, max_src_seq_len - L)), 0) for L in src_len])\n",
    "tgt_seq = torch.cat(\n",
    "    [torch.unsqueeze(F.pad(torch.randint(1, max_num_tgt_words, (L,)), (0, max_tgt_seq_len - L)), 0) for L in tgt_len])\n",
    "\n",
    "# Construct word embedding\n",
    "src_embedding_table = nn.Embedding(max_num_src_words + 1, model_dim)\n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words + 1, model_dim)\n",
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding = tgt_embedding_table(tgt_seq)\n",
    "\n",
    "# Construct position embedding\n",
    "pos_mat = torch.arange(max_position_len).reshape((-1, 1))\n",
    "i_mat = torch.pow(10000, torch.arange(0, 8, 2).reshape((1, -1)) / model_dim)\n",
    "pe_embedding_table = torch.zeros(max_position_len, model_dim)\n",
    "pe_embedding_table[:, 0::2] = torch.sin(pos_mat / i_mat)\n",
    "pe_embedding_table[:, 1::2] = torch.cos(pos_mat / i_mat)\n",
    "\n",
    "pe_embedding = nn.Embedding(max_position_len, model_dim)\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table, requires_grad=False)\n",
    "\n",
    "src_positions = torch.arange(max_src_seq_len).expand(batch_size, max_src_seq_len)\n",
    "src_pe = pe_embedding(src_positions)\n",
    "src_pe_embedding = src_embedding + src_pe\n",
    "\n",
    "tgt_positions = torch.arange(max_tgt_seq_len).expand(batch_size, max_tgt_seq_len)\n",
    "tgt_pe = pe_embedding(tgt_positions)\n",
    "tgt_pe_embedding = tgt_embedding + tgt_pe\n",
    "\n",
    "# self-attention mask of encoder\n",
    "# The shape of mask: [batch_size, max_src_len, max_src_len], value: 1 or -inf\n",
    "valid_encoder_pos = torch.unsqueeze(\n",
    "    torch.cat([torch.unsqueeze(F.pad(torch.ones(L), (0, max(src_len) - L)), 0) for L in src_len]), 2)\n",
    "valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos, valid_encoder_pos.transpose(1, 2))\n",
    "invalid_encoder_pos_matrix = 1 - valid_encoder_pos_matrix\n",
    "mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.bool)\n",
    "\n",
    "score = torch.randn(batch_size, max(src_len), max(src_len))\n",
    "masked_score = score.masked_fill(mask_encoder_self_attention, -1e9)\n",
    "prob = F.softmax(masked_score, -1)\n",
    "\n",
    "print(prob)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-13T12:13:24.181315Z",
     "start_time": "2025-04-13T12:13:24.157955Z"
    }
   },
   "id": "c26cf78dfccc75cc",
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2035, 0.7965, 0.0000, 0.0000],\n",
      "         [0.5029, 0.2976, 0.1996, 0.0000],\n",
      "         [0.0221, 0.1329, 0.4752, 0.3698]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5493, 0.4507, 0.0000, 0.0000],\n",
      "         [0.1614, 0.3774, 0.4612, 0.0000],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500]]])\n"
     ]
    }
   ],
   "source": [
    "# mask of intra-attention\n",
    "valid_decoder_pos = torch.unsqueeze(\n",
    "    torch.cat([torch.unsqueeze(F.pad(torch.ones(L), (0, max(tgt_len) - L)), 0) for L in tgt_len]), 2)\n",
    "valid_cross_pos_matrix = torch.bmm(valid_decoder_pos, valid_encoder_pos.transpose(1, 2))\n",
    "invalid_cross_pos_matrix = 1 - valid_cross_pos_matrix\n",
    "mask_cross_attention = invalid_cross_pos_matrix.to(torch.bool)\n",
    "\n",
    "valid_decoder_tril_matrix = torch.cat(\n",
    "    [torch.unsqueeze(F.pad(torch.tril(torch.ones((L, L))), (0, max(tgt_len) - L, 0, max(tgt_len) - L)), 0) for L in\n",
    "     tgt_len])\n",
    "invalid_decoder_tri_matrix = 1 - valid_decoder_tril_matrix\n",
    "invalid_decoder_tri_matrix = invalid_decoder_tri_matrix.to(torch.bool)\n",
    "\n",
    "score = torch.randn(batch_size, max(tgt_len), max(tgt_len))\n",
    "masked_score = score.masked_fill(invalid_decoder_tri_matrix, -1e9)\n",
    "prob = F.softmax(masked_score, -1)\n",
    "print(prob)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-14T02:14:06.759545Z",
     "start_time": "2025-04-14T02:14:06.736697Z"
    }
   },
   "id": "765f445d0eeb716d",
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, attn_mask):\n",
    "    score = torch.bmm(Q, K.transpose(-2, -1)) / torch.sqrt(model_dim)\n",
    "    masked_score = score.masked_fill(attn_mask, -1e9)\n",
    "    prob = F.softmax(masked_score, -1)\n",
    "    context = torch.bmm(prob, V)\n",
    "    return context"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eaf21ebee88fc5dc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
