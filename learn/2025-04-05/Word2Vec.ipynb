{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3761deef35066b0c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## One-Hot Encoding\n",
    "One-Hot Encoding is a method for converting categorical variables into a numerical format, making it easier for machine learning algorithms to process the data. In categorical variables, each value represents a distinct category, and there is no inherent order or numerical relationship between them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d780fa589bdbdd9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## N-gram\n",
    "Given a sentence containing m words (w₁, w₂, ..., wₘ), the probability of this sentence is calculated as:\n",
    "$$P(sen = (w_1, w_2, \\ldots, w_m)) = P(w_1)P(w_2 | w_1)P(w_3 | w_2, w_1) \\ldots P(w_m | w_{m-1}, \\ldots, w_1)$$\n",
    "Typically, language models aim to maximize the conditional probability: P(wₜ | w₁, w₂, ..., wₜ₋₁). However, due to the recency effect, the current word is primarily correlated with its nearest n preceding words (usually n <= 5), rather than being dependent on all previous words.\n",
    "Therefore, the above formula can be approximated as:\n",
    "$$P(w_t | w_1, w_2, \\dots, w_{t-1}) = P(w_t | w_{t-1}, w_{t-2}, \\dots, w_{t-(n-1)})$$\n",
    "\n",
    "However, the N-gram model still has its limitations:\n",
    "* First, due to the exponential growth of parameter space, it cannot effectively handle longer contextual dependencies (when N > 3).\n",
    "* Second, it fails to capture the intrinsic relationships between words."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc7fecb5141b4fae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is Word Embedding?\n",
    "**Word embedding** is a technique in Natural Language Processing (NLP) that represents words (or phrases, sentences) as **low-dimensional, dense vectors** in a continuous vector space. These vectors capture semantic and syntactic relationships between words, allowing machines to process language more effectively. \n",
    "\n",
    "## Why Use Word Embeddings?\n",
    "Traditional methods (e.g., one-hot encoding) suffer from:\n",
    "* High dimensionality (e.g., 10,000D for a 10K-word vocabulary).\n",
    "* No semantic meaning (all word vectors are orthogonal).\n",
    "\n",
    "Word embeddings solve these by:\n",
    "* Lower dimensions (e.g., 300D).\n",
    "* Preserving semantic relationships.\n",
    "\n",
    "## Popular Word Embedding Models\n",
    "1. **Word2Vec**\n",
    "* Uses **CBOW** (predicts a word from context) or **Skip-gram** (predicts context from a word).\n",
    "* Trained on large text corpora.\n",
    "2. **GloVe** (Global Vectors)\n",
    "* combines **global co-occurrence statistics** with loca context.\n",
    "3. ...."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "122b354a5b098dfd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CBOW (Continuous Bag of Words)\n",
    "**CBOW** is one of the two main architectures in **Word2Vec** (the other being **Skip-gram**). It predicts a **target word** based on its **context words** (surrounding words in a fixed window).\n",
    "\n",
    "## Training Steps of the CBOW Model\n",
    "The basic training stops of the **CBOW (Continuous Bag of Words)** model include:\n",
    "1. **input Representation**\n",
    "* Represent the **context words** as **one-hot vectors**, where the vocabulary size is **V**, and the number of context words is **C**.\n",
    "2. **Projection to Hidden Layer**\n",
    "* Multiply each context word's **one-hot vector** by the **input-to-hidden weight matrix W** (of size **V * N**, where N is the embedding dimension). \n",
    "3. **Hidden Layer Vector Calculation**\n",
    "* Sum all the resulting vectors from the previous step and take their **average** to produce the **hidden layer vector** (also N-dimensional).\n",
    "4. **Output Layer Projection**\n",
    "* Multiply the hidden layer vector by the **hidden-to-output weight matrix W'** (of size **N * V**) to generate a V-dimensional score vector.\n",
    "5. **Prediction via Softmax**\n",
    "* Apply the **softmax activation** to the score vector to obtain a **probability distribution** over the vocabulary.\n",
    "* The word with the **highest probability** is selected as the predicted **target word**."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bb3c492c738fc47"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.13227147e-03 -4.45733406e-03 -1.06835726e-03  1.00636482e-03\n",
      " -1.91113955e-04  1.14817743e-03  6.11386076e-03 -2.02715401e-05\n",
      " -3.24596534e-03 -1.51072862e-03  5.89729892e-03  1.51410222e-03\n",
      " -7.24261976e-04  9.33324732e-03 -4.92128357e-03 -8.38409644e-04\n",
      "  9.17541143e-03  6.74942741e-03  1.50285603e-03 -8.88256077e-03\n",
      "  1.14874600e-03 -2.28825561e-03  9.36823711e-03  1.20992784e-03\n",
      "  1.49006362e-03  2.40640994e-03 -1.83600665e-03 -4.99963388e-03\n",
      "  2.32429506e-04 -2.01418041e-03  6.60093315e-03  8.94012302e-03\n",
      " -6.74754381e-04  2.97701475e-03 -6.10765442e-03  1.69932481e-03\n",
      " -6.92623248e-03 -8.69402662e-03 -5.90020278e-03 -8.95647518e-03\n",
      "  7.27759488e-03 -5.77203138e-03  8.27635173e-03 -7.24354526e-03\n",
      "  3.42167495e-03  9.67499893e-03 -7.78544787e-03 -9.94505733e-03\n",
      " -4.32914635e-03 -2.68313056e-03 -2.71289347e-04 -8.83155130e-03\n",
      " -8.61755759e-03  2.80021061e-03 -8.20640661e-03 -9.06933658e-03\n",
      " -2.34046578e-03 -8.63180775e-03 -7.05664977e-03 -8.40115082e-03\n",
      " -3.01328895e-04 -4.56429832e-03  6.62717456e-03  1.52716041e-03\n",
      " -3.34147573e-03  6.10897178e-03 -6.01328490e-03 -4.65616956e-03\n",
      " -7.20750913e-03 -4.33658017e-03 -1.80932996e-03  6.48964290e-03\n",
      " -2.77039292e-03  4.91896737e-03  6.90444233e-03 -7.46370573e-03\n",
      "  4.56485013e-03  6.12697843e-03 -2.95447465e-03  6.62502181e-03\n",
      "  6.12587947e-03 -6.44348515e-03 -6.76455162e-03  2.53895880e-03\n",
      " -1.62381888e-03 -6.06512791e-03  9.49920900e-03 -5.13014663e-03\n",
      " -6.55409694e-03 -1.19885204e-04 -2.70142802e-03  4.44400299e-04\n",
      " -3.53745813e-03 -4.19330609e-04 -7.08615757e-04  8.22820642e-04\n",
      "  8.19481723e-03 -5.73670724e-03 -1.65952800e-03  5.57160750e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [[\"I\", \"love\", \"natural\", \"language\", \"processing\"], \n",
    "             [\"CBOW\", \"is\", \"a\", \"Word2Vec\", \"model\"]]\n",
    "\n",
    "# Train CBOW (sg=0 means CBOW; sg=1 means Skip-gram)\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0, workers=4)\n",
    "\n",
    "# Get word vector for \"language\"\n",
    "vector = model.wv[\"language\"]\n",
    "print(vector)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-05T07:04:31.625661Z",
     "start_time": "2025-04-05T07:04:31.009605Z"
    }
   },
   "id": "dc393cb45c007a3e",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('model', 0.09291722625494003), ('language', 0.004842500668019056)]\n"
     ]
    }
   ],
   "source": [
    "# Find similar words\n",
    "similar_words = model.wv.most_similar(\"natural\", topn=2)\n",
    "print(similar_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-05T07:04:43.095528Z",
     "start_time": "2025-04-05T07:04:43.090707Z"
    }
   },
   "id": "758f2255101223e8",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Skip-gram\n",
    "Skip-gram is one of the two main architectures in **Word2Vec** (the other being CBOW). Unlike **CBOW**, which is predicts a target word from its context, Skip-gram **does the opposite**:it takes a **center word** as input and predicts its surrounding **context words**. This approach excels at capturing **semantic relationships** and works particularly well for **rare words** and complex linguistic patterns."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c60b9b2ba4aaefff"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.1681199e-03 -4.4430327e-03  8.9854337e-03  8.2536647e-03\n",
      " -4.4352221e-03  3.0310510e-04  4.2744912e-03 -3.9263200e-03\n",
      " -5.5599655e-03 -6.5123225e-03 -6.7073823e-04 -2.9592158e-04\n",
      "  4.4630850e-03 -2.4740540e-03 -1.7260908e-04  2.4618758e-03\n",
      "  4.8675989e-03 -3.0808449e-05 -6.3394094e-03 -9.2608072e-03\n",
      "  2.6657581e-05  6.6618943e-03  1.4660227e-03 -8.9665223e-03\n",
      " -7.9386048e-03  6.5519023e-03 -3.7856805e-03  6.2549924e-03\n",
      " -6.6810320e-03  8.4796622e-03 -6.5163244e-03  3.2880199e-03\n",
      " -1.0569858e-03 -6.7875278e-03 -3.2875966e-03 -1.1614120e-03\n",
      " -5.4709399e-03 -1.2113475e-03 -7.5633135e-03  2.6466595e-03\n",
      "  9.0701487e-03 -2.3772502e-03 -9.7651005e-04  3.5135616e-03\n",
      "  8.6650876e-03 -5.9218528e-03 -6.8875779e-03 -2.9329848e-03\n",
      "  9.1476962e-03  8.6626766e-04 -8.6784009e-03 -1.4469790e-03\n",
      "  9.4794659e-03 -7.5494875e-03 -5.3580985e-03  9.3165627e-03\n",
      " -8.9737261e-03  3.8259076e-03  6.6544057e-04  6.6607012e-03\n",
      "  8.3127534e-03 -2.8507852e-03 -3.9923131e-03  8.8979173e-03\n",
      "  2.0896459e-03  6.2489416e-03 -9.4457148e-03  9.5901238e-03\n",
      " -1.3483083e-03 -6.0521150e-03  2.9925345e-03 -4.5661093e-04\n",
      "  4.7064926e-03 -2.2830211e-03 -4.1378425e-03  2.2778988e-03\n",
      "  8.3543835e-03 -4.9956059e-03  2.6686788e-03 -7.9905549e-03\n",
      " -6.7733466e-03 -4.6766878e-04 -8.7677278e-03  2.7894378e-03\n",
      "  1.5985954e-03 -2.3196924e-03  5.0037908e-03  9.7487867e-03\n",
      "  8.4542679e-03 -1.8802249e-03  2.0581519e-03 -4.0036892e-03\n",
      " -8.2414057e-03  6.2779556e-03 -1.9491815e-03 -6.6620467e-04\n",
      " -1.7713320e-03 -4.5356657e-03  4.0617096e-03 -4.2701806e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [\n",
    "    [\"I\", \"love\", \"natural\", \"language\", \"processing\"],\n",
    "    [\"Skip-gram\", \"is\", \"powerful\", \"for\", \"word\", \"embeddings\"]\n",
    "]\n",
    "\n",
    "# Train Skip-gram (sg=1)\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1, workers=4)\n",
    "\n",
    "# Get word vector\n",
    "vector = model.wv[\"language\"]\n",
    "print(vector)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-05T07:04:52.385373Z",
     "start_time": "2025-04-05T07:04:52.372312Z"
    }
   },
   "id": "a73c093b90afebd8",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('for', 0.19912061095237732), ('powerful', 0.07497558742761612)]\n"
     ]
    }
   ],
   "source": [
    "# Find similar words\n",
    "print(model.wv.most_similar(\"natural\", topn=2))\n",
    "# Output: [('language', 0.88), ('processing', 0.82)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-05T07:04:54.741032Z",
     "start_time": "2025-04-05T07:04:54.738304Z"
    }
   },
   "id": "e386c0a908636e90",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "### Key Innovations\n",
    "1. Dense Vector Representations\n",
    "* Transforms words from sparse one-hot encodings (e.g., 50,000D) to compact 100-300D continuous vectors.\n",
    "2. Semantic Arithmetic\n",
    "* Enables vector operations that reflect linguistic relationships\n",
    "\n",
    "### Architecture Comparison\n",
    "| Feature  | CBOW                               | Skip-gram                       |\n",
    "|----------|------------------------------------|---------------------------------|\n",
    "| Approach | Predicts center from context       | Predicts context from center    |\n",
    "| Speed    | Faster (better for frequent words) | Slower (excels with rare words) |\n",
    "| Strength | Syntactic patterns                 | Semantic relationships          |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8af32132ad31c801"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13887984\n",
      "[('语言', 0.17018885910511017), ('自然', 0.06408978998661041)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [[\"自然\", \"语言\", \"处理\"], [\"深度学习\", \"改变\", \"世界\"]]\n",
    "\n",
    "# Train model (default Skip-gram)\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "# Get word vector\n",
    "vector = model.wv[\"自然\"]\n",
    "\n",
    "# Similarity\n",
    "print(model.wv.similarity(\"自然\", \"语言\"))\n",
    "print(model.wv.most_similar(\"深度学习\", topn=2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-05T07:14:30.270175Z",
     "start_time": "2025-04-05T07:14:30.258211Z"
    }
   },
   "id": "e0f54012a3616199",
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
